{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a7d72e1-8d61-4596-90e9-83cf8307c585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best eval/accuracy: 0.646500 (step=1750)\n",
      "From: mmlu/eval_metrics.jsonl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json, re, gzip, glob\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Tuple, Union\n",
    "\n",
    "\n",
    "# Matches \"key=value\" or \"key: value\"\n",
    "_KV_SEP = re.compile(r'^\\s*([^:=\\s]+)\\s*[:=]\\s*(.+?)\\s*$')\n",
    "# Matches \"key<no-separator>number\", e.g., \"eval/accuracy0.5605\", \"step200\"\n",
    "_KV_CAT = re.compile(r'^\\s*([^\\d\\-\\.+]+)\\s*([-+]?\\d+(?:\\.\\d+)?(?:[eE][-+]?\\d+)?)\\s*$')\n",
    "\n",
    "def _open(path: Union[str, Path]):\n",
    "    p = str(path)\n",
    "    return gzip.open(p, \"rt\", encoding=\"utf-8\") if p.endswith(\".gz\") else open(p, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "def _try_float(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return x\n",
    "\n",
    "def _parse_line(line: str):\n",
    "    \"\"\"Return a dict for a single line (either a full JSON object or a single key/value),\n",
    "    or None if not parseable by known patterns.\n",
    "    \"\"\"\n",
    "    s = line.strip()\n",
    "    if not s:\n",
    "        return None\n",
    "\n",
    "    # Try JSON per-line\n",
    "    try:\n",
    "        obj = json.loads(s)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj  # entire record\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    m = _KV_SEP.match(s)\n",
    "    if m:\n",
    "        k, v = m.group(1), _try_float(m.group(2))\n",
    "        return {k: v}\n",
    "\n",
    "    m = _KV_CAT.match(s)\n",
    "    if m:\n",
    "        k, v = m.group(1), _try_float(m.group(2))\n",
    "        return {k: v}\n",
    "\n",
    "    return None\n",
    "\n",
    "def load_records(path: Union[str, Path]) -> Iterable[dict]:\n",
    "    \"\"\"Yield dict records. Handles JSONL and flat key/value lines grouped by step/blank lines.\"\"\"\n",
    "    rec = {}\n",
    "    with _open(path) as f:\n",
    "        for raw in f:\n",
    "            line = raw.strip()\n",
    "            if not line:\n",
    "                if rec:\n",
    "                    yield rec\n",
    "                    rec = {}\n",
    "                continue\n",
    "\n",
    "            parsed = _parse_line(line)\n",
    "            if parsed is None:\n",
    "                continue\n",
    "\n",
    "            # Full JSON object line: flush current, yield full, continue\n",
    "            if len(parsed) > 1:\n",
    "                if rec:\n",
    "                    yield rec; rec = {}\n",
    "                yield parsed\n",
    "                continue\n",
    "\n",
    "            # Single k/v\n",
    "            k, v = next(iter(parsed.items()))\n",
    "            if k == \"step\" and rec:\n",
    "                # new record begins\n",
    "                yield rec\n",
    "                rec = {}\n",
    "            rec[k] = v\n",
    "\n",
    "    if rec:\n",
    "        yield rec\n",
    "\n",
    "def best_eval_accuracy(path: Union[str, Path], field: str = \"eval/accuracy\"):\n",
    "    \"\"\"Return (accuracy, step, record) for the best metric in a single file, or None if not found.\"\"\"\n",
    "    best = None\n",
    "    for rec in load_records(path):\n",
    "        acc = rec.get(field, None)\n",
    "        if isinstance(acc, (int, float)):\n",
    "            step = rec.get(\"step\")\n",
    "            if (best is None) or (acc > best[0]):\n",
    "                best = (acc, step, rec)\n",
    "    return best\n",
    "\n",
    "def avg_eval_accuracy(path: Union[str, Path], field: str = \"eval/accuracy\"):\n",
    "    \"\"\"Return (average_accuracy, count, records) for all numeric entries in a single file, or None if none found.\"\"\"\n",
    "    values = []\n",
    "    records = []\n",
    "\n",
    "    for rec in load_records(path):\n",
    "        acc = rec.get(field, None)\n",
    "        if isinstance(acc, (int, float)):\n",
    "            values.append(acc)\n",
    "            records.append(rec)\n",
    "\n",
    "    if not values:\n",
    "        return None\n",
    "\n",
    "    avg = sum(values) / len(values)\n",
    "    return avg, len(values), records\n",
    "\n",
    "def leaderboard(paths, field: str = \"eval/accuracy\", min_total: int = 0):\n",
    "    \"\"\"For multiple files/globs, return a sorted list of (acc, step, file) by descending acc.\"\"\"\n",
    "    files = []\n",
    "    for p in paths:\n",
    "        matches = glob.glob(p)\n",
    "        if matches:\n",
    "            files.extend(matches)\n",
    "    files = sorted(set(files))\n",
    "    rows = []\n",
    "    for fp in files:\n",
    "        best = best_eval_accuracy(fp, field=field)\n",
    "        if best is not None:\n",
    "            acc, step, rec = best\n",
    "            rows.append((acc, step, fp))\n",
    "    rows.sort(key=lambda r: r[0], reverse=True)\n",
    "    return rows\n",
    "\n",
    "metrics_file = \"mmlu/eval_metrics.jsonl\"   # <-- change this\n",
    "metric_field = \"eval/accuracy\"\n",
    "\n",
    "best = best_eval_accuracy(metrics_file, field=metric_field)\n",
    "if best is None:\n",
    "    print(f\"No numeric '{metric_field}' found in {metrics_file}\")\n",
    "else:\n",
    "    acc, step, rec = best\n",
    "    step_str = f\"step={int(step)}\" if isinstance(step, (int, float)) else f\"step={step}\" if step is not None else \"step=?\"\n",
    "    print(f\"Best {metric_field}: {acc:.6f} ({step_str})\\nFrom: {metrics_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b02bf30-3f92-4b90-96a7-5342b93062d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 6 runs.\n",
      "\n",
      "Run 01: 4 records | steps 500→2000 | expected-steps-present 4/41 | best acc=0.690071 @ step=2000| avg acc 0.6783\n",
      "Run 02: 4 records | steps 500→2000 | expected-steps-present 4/41 | best acc=0.696302 @ step=2000| avg acc 0.6837\n",
      "Run 03: 4 records | steps 500→2000 | expected-steps-present 4/41 | best acc=0.682592 @ step=1500| avg acc 0.6763\n",
      "Run 04: 4 records | steps 500→2000 | expected-steps-present 4/41 | best acc=0.659327 @ step=1500| avg acc 0.6515\n",
      "Run 05: 4 records | steps 500→2000 | expected-steps-present 4/41 | best acc=0.631076 @ step=2000| avg acc 0.6272\n",
      "Run 06: 4 records | steps 500→2000 | expected-steps-present 4/41 | best acc=0.579560 @ step=1500| avg acc 0.5735\n",
      "Saved 6 run files to /workspace/AUX_DPO/isolated_runs\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "EXPECTED_STEPS = set(range(50, 2500 + 1, 50))  # 50,100,...,2000\n",
    "\n",
    "def load_records(jsonl_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load JSONL file into a list of dicts; ignore malformed lines.\"\"\"\n",
    "    recs = []\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rec = json.loads(line)\n",
    "                if isinstance(rec, dict) and \"step\" in rec:\n",
    "                    rec[\"_row\"] = i  # keep original order index for debugging\n",
    "                    recs.append(rec)\n",
    "            except json.JSONDecodeError:\n",
    "                # Skip malformed lines silently; or log if you prefer\n",
    "                pass\n",
    "    return recs\n",
    "\n",
    "def split_into_runs(records: List[Dict[str, Any]]) -> List[List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Split a chronologically ordered list of eval records into runs.\n",
    "    A new run starts when:\n",
    "      - step decreases vs previous step, or\n",
    "      - step == 50 and current run already has items (common restart signature).\n",
    "    \"\"\"\n",
    "    runs: List[List[Dict[str, Any]]] = []\n",
    "    current: List[Dict[str, Any]] = []\n",
    "    prev_step = None\n",
    "\n",
    "    for rec in records:\n",
    "        step = rec.get(\"step\")\n",
    "        if prev_step is None:\n",
    "            current = [rec]\n",
    "        else:\n",
    "            # start of a new run?\n",
    "            if (isinstance(step, (int, float)) and isinstance(prev_step, (int, float)) and step < prev_step) \\\n",
    "               or (step == 50 and current):\n",
    "                # close old\n",
    "                if current:\n",
    "                    runs.append(current)\n",
    "                current = [rec]\n",
    "            else:\n",
    "                current.append(rec)\n",
    "        prev_step = step\n",
    "\n",
    "    if current:\n",
    "        runs.append(current)\n",
    "    return runs\n",
    "\n",
    "def avg_eval_accuracy(run: List[Dict[str, Any]], field: str = \"eval/accuracy\"):\n",
    "    \"\"\"Return (average_accuracy, count, records) for all numeric entries in a single file, or None if none found.\"\"\"\n",
    "    values = []\n",
    "    \n",
    "\n",
    "    for rec in run:\n",
    "        acc = rec.get(field)\n",
    "        if isinstance(acc, (int, float)):\n",
    "            values.append(acc)\n",
    "    \n",
    "    if not values:\n",
    "        return None\n",
    "\n",
    "    avg = sum(values) / len(values)\n",
    "    return avg\n",
    "\n",
    "def best_eval_accuracy(run: List[Dict[str, Any]], field: str = \"eval/accuracy\") -> Tuple[float, Any, Dict[str, Any]]:\n",
    "    \"\"\"Return (best_acc, step_of_best, record_of_best) for a run.\"\"\"\n",
    "    best = (-float(\"inf\"), None, None)\n",
    "    for rec in run:\n",
    "        acc = rec.get(field)\n",
    "        if isinstance(acc, (int, float)) and acc > best[0]:\n",
    "            best = (acc, rec.get(\"step\"), rec)\n",
    "    return best  # (acc, step, full_record)\n",
    "\n",
    "def summarize_runs(runs: List[List[Dict[str, Any]]]) -> None:\n",
    "    print(f\"Detected {len(runs)} runs.\\n\")\n",
    "    for i, run in enumerate(runs, 1):\n",
    "        steps = [r.get(\"step\") for r in run if isinstance(r.get(\"step\"), (int, float))]\n",
    "        step_min = min(steps) if steps else None\n",
    "        step_max = max(steps) if steps else None\n",
    "        # how many expected step slots present\n",
    "        present = sum(1 for s in steps if s in EXPECTED_STEPS)\n",
    "        best_acc, best_step, _ = best_eval_accuracy(run)\n",
    "        avg = avg_eval_accuracy(run)\n",
    "        print(f\"Run {i:02d}: {len(run)} records | steps {step_min}→{step_max} \"\n",
    "              f\"| expected-steps-present {present}/41 | best acc={best_acc:.6f} @ step={best_step}\"\n",
    "              f\"| avg acc {avg:.4f}\")\n",
    "\n",
    "def save_runs(runs: List[List[Dict[str, Any]]], out_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Save each run to its own JSONL file: run_01.jsonl, run_02.jsonl, ...\n",
    "    \"\"\"\n",
    "    out = Path(out_dir)\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "    for i, run in enumerate(runs, 1):\n",
    "        p = out / f\"run_{i:02d}.jsonl\"\n",
    "        with p.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            for rec in run:\n",
    "                # drop helper keys\n",
    "                rec_to_write = {k: v for k, v in rec.items() if not k.startswith(\"_\")}\n",
    "                f.write(json.dumps(rec_to_write, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"Saved {len(runs)} run files to {out.resolve()}\")\n",
    "\n",
    "# ---------- usage ----------\n",
    "# If your data is in a file, set this:\n",
    "jsonl_path = \"llama_mmlu_in/eval_metrics.jsonl\"\n",
    "# If it's in-memory text, write it to a temp file first.\n",
    "\n",
    "records = load_records(jsonl_path)\n",
    "runs = split_into_runs(records)\n",
    "summarize_runs(runs)\n",
    "save_runs(runs, out_dir=\"isolated_runs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20efac26-56cb-4925-9103-9a145257e265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5b3dab-9b48-4e20-b011-20d6813ae988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
