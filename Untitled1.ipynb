{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "064d8ff4-0760-4be0-8594-69ab29a39689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Using the `SDPA` attention implementation on multi-gpu setup with ROCM may lead to performance issues due to the FA backend. Disabling it to use alternative backends.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f806b096c740ed9b38bf902868f1b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2412: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py:83: UserWarning: 1Torch was not compiled with memory efficient attention. (Triggered internally at /var/lib/jenkins/pytorch/aten/src/ATen/native/transformers/hip/sdp_utils.cpp:517.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------\n",
    "# Prompting & parsing\n",
    "# --------------------------\n",
    "LETTERS = [chr(ord('A') + i) for i in range(26)]\n",
    "\n",
    "def format_options(options: List[str]) -> str:\n",
    "    lines = []\n",
    "    for i, opt in enumerate(options):\n",
    "        lines.append(f\"{LETTERS[i]}. {opt}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def build_prompt(question: str, options: List[str]) -> str:\n",
    "    letters = \", \".join(LETTERS[:len(options)])\n",
    "    return (\n",
    "        \"You are given a multiple-choice question.\\n\"\n",
    "        \"Choose the correct option and answer with a single CAPITAL LETTER only.\\n\"\n",
    "        f\"Valid answers: {{{letters}}}\\n\\n\"\n",
    "        f\"Question:\\n{question}\\n\\n\"\n",
    "        f\"Options:\\n{format_options(options)}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "# Prefer word-boundary matches; avoid picking 'A' in 'Answer'\n",
    "LETTER_TOKEN_RE = re.compile(r\"\\b([A-Z])\\b\")\n",
    "PAREN_LETTER_RE = re.compile(r\"\\(([A-Z])\\)\")\n",
    "\n",
    "def parse_letter(text: str, k: int) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract the chosen letter from model output. Only return a letter in {A.. up to k}.\n",
    "    Order of attempts:\n",
    "      1) exact single-letter (strict)\n",
    "      2) any standalone single-letter token (word boundaries)\n",
    "      3) common '(C)' parenthetical pattern\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "\n",
    "    # 1) strict\n",
    "    m = re.match(r\"^\\s*([A-Z])\\s*$\", text)\n",
    "    if m:\n",
    "        ch = m.group(1)\n",
    "        if 0 <= (ord(ch) - 65) < k:\n",
    "            return ch\n",
    "\n",
    "    # 2) standalone tokens\n",
    "    tokens = LETTER_TOKEN_RE.findall(text)\n",
    "    for ch in tokens:\n",
    "        if 0 <= (ord(ch) - 65) < k:\n",
    "            return ch\n",
    "\n",
    "    # 3) parenthetical like '(C)'\n",
    "    m = PAREN_LETTER_RE.search(text)\n",
    "    if m:\n",
    "        ch = m.group(1)\n",
    "        if 0 <= (ord(ch) - 65) < k:\n",
    "            return ch\n",
    "\n",
    "    return None\n",
    "\n",
    "def gold_letter_from_row(row: Dict[str, Any]) -> str:\n",
    "    if \"answer_index\" in row and row[\"answer_index\"] is not None:\n",
    "        return LETTERS[int(row[\"answer_index\"])]\n",
    "    return str(row[\"answer\"]).strip().upper()\n",
    "\n",
    "# --------------------------\n",
    "# Data loading\n",
    "# --------------------------\n",
    "import json\n",
    "\n",
    "def load_jsonl(path: str) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "def load_dataset(path: str) -> List[Dict[str, Any]]:\n",
    "    if path.endswith(\".jsonl\"):\n",
    "        return load_jsonl(path)\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        obj = json.load(f)\n",
    "    if isinstance(obj, list):\n",
    "        return obj\n",
    "    raise ValueError(\"JSON must be a list of rows for .json\")\n",
    "\n",
    "# --------------------------\n",
    "# Gen helpers (device-safe)\n",
    "# --------------------------\n",
    "def _is_sharded(model: torch.nn.Module) -> bool:\n",
    "    # Present when loaded with accelerate/device_map=\"auto\"\n",
    "    return getattr(model, \"hf_device_map\", None) is not None\n",
    "\n",
    "def _primary_device(model: torch.nn.Module) -> torch.device:\n",
    "    try:\n",
    "        return next(model.parameters()).device\n",
    "    except StopIteration:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_letters(\n",
    "    model, tokenizer, prompts: List[str],\n",
    "    max_new_tokens: int = 4,\n",
    "    temperature: float = 0.0,\n",
    "    top_p: float = 1.0,\n",
    "    batch_size: int = 8,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Device-safe batching:\n",
    "      - If model is sharded (device_map=\"auto\"), DO NOT move batches to a single device.\n",
    "      - Else, move the batch to the modelâ€™s primary device.\n",
    "    \"\"\"\n",
    "    out_texts: List[str] = []\n",
    "    sharded = _is_sharded(model)\n",
    "    primary = _primary_device(model)\n",
    "\n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        batch = prompts[i:i+batch_size]\n",
    "        toks = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        if not sharded:\n",
    "            toks = {k: v.to(primary) for k, v in toks.items()}\n",
    "\n",
    "        gen = model.generate(\n",
    "            **toks,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=(temperature > 0.0),\n",
    "            temperature=max(temperature, 1e-8),\n",
    "            top_p=top_p,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "        )\n",
    "        # Slice out the newly generated tokens only\n",
    "        new_tokens = gen[:, toks[\"input_ids\"].shape[1]:]\n",
    "        texts = tokenizer.batch_decode(new_tokens, skip_special_tokens=True)\n",
    "        out_texts.extend([t.strip() for t in texts])\n",
    "\n",
    "    return out_texts\n",
    "\n",
    "# --------------------------\n",
    "# Evaluation (revised)\n",
    "# --------------------------\n",
    "def evaluate(\n",
    "    model_name: str,                  # path to local dir OR HF id\n",
    "    data_path: str,\n",
    "    max_samples: int = -1,\n",
    "    batch_size: int = 8,\n",
    "    max_new_tokens: int = 4,\n",
    "    temperature: float = 0.0,\n",
    "    top_p: float = 1.0,\n",
    "    local_files_only: bool = False,\n",
    ") -> Dict[str, Any]:\n",
    "\n",
    "    # Tokenizer: left padding for decoder-only generation\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, local_files_only=local_files_only)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Model (single GPU/CPU or sharded)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else None,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        local_files_only=local_files_only,\n",
    "    ).eval()\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Data\n",
    "    rows = load_dataset(data_path)\n",
    "    if max_samples > 0:\n",
    "        rows = rows[:max_samples]\n",
    "\n",
    "    prompts, k_list, gold_letters, subjects = [], [], [], []\n",
    "    for row in rows:\n",
    "        question = row[\"question\"]\n",
    "        options  = row[\"options\"]\n",
    "        k = len(options)\n",
    "        prompts.append(build_prompt(question, options))\n",
    "        k_list.append(k)\n",
    "        gold_letters.append(gold_letter_from_row(row))\n",
    "        subjects.append(row.get(\"subject\", None))\n",
    "\n",
    "    # Generation\n",
    "    gens = generate_letters(\n",
    "        model, tokenizer, prompts,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature, top_p=top_p,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    # Parse predictions\n",
    "    preds: List[Optional[str]] = [parse_letter(txt, k) for txt, k in zip(gens, k_list)]\n",
    "\n",
    "    # Accuracy\n",
    "    correct_flags = [(p == g) for p, g in zip(preds, gold_letters)]\n",
    "    overall = sum(bool(x) for x in correct_flags) / max(1, len(correct_flags))\n",
    "\n",
    "    # Per-subject accuracy (if subject provided)\n",
    "    per_subject_hits: Dict[str, Tuple[int, int]] = {}\n",
    "    for ok, subj in zip(correct_flags, subjects):\n",
    "        if subj is None:\n",
    "            continue\n",
    "        hit, tot = per_subject_hits.get(subj, (0, 0))\n",
    "        per_subject_hits[subj] = (hit + int(ok), tot + 1)\n",
    "    per_subject_acc = {k: (h / t) for k, (h, t) in per_subject_hits.items() if t > 0}\n",
    "\n",
    "    return {\n",
    "        \"overall_accuracy\": overall,\n",
    "        \"n_examples\": len(rows),\n",
    "        \"per_subject_accuracy\": per_subject_acc,\n",
    "        \"predictions\": preds,\n",
    "        \"gold\": gold_letters,\n",
    "        \"raw_generations\": gens,\n",
    "    }\n",
    "\n",
    "res = evaluate(\n",
    "    model_name=\"./llama8b_mmlu_dpop_id/checkpoint-2407\",  # local folder with model + tokenizer\n",
    "    data_path=\"mmlu_pro_test.jsonl\",\n",
    "    max_samples=-1,\n",
    "    batch_size=8,\n",
    "    max_new_tokens=4,\n",
    "    temperature=0.0,\n",
    "    top_p=1.0,\n",
    "    local_files_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a708ae4f-cfda-4347-a368-a2e55ce25712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall accuracy (win-rate): 36.25%  on N=12032\n",
      "\n",
      "Per-subject accuracy:\n",
      "  biology: 61.23%\n",
      "  business: 28.52%\n",
      "  chemistry: 26.94%\n",
      "  computer science: 35.37%\n",
      "  economics: 48.46%\n",
      "  engineering: 32.92%\n",
      "  health: 49.76%\n",
      "  history: 43.04%\n",
      "  law: 31.79%\n",
      "  math: 22.72%\n",
      "  other: 38.10%\n",
      "  philosophy: 33.47%\n",
      "  physics: 26.40%\n",
      "  psychology: 53.88%\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nOverall accuracy (win-rate): {res['overall_accuracy']*100:.2f}%  on N={res['n_examples']}\")\n",
    "if res[\"per_subject_accuracy\"]:\n",
    "    print(\"\\nPer-subject accuracy:\")\n",
    "    for k in sorted(res[\"per_subject_accuracy\"].keys()):\n",
    "        print(f\"  {k}: {res['per_subject_accuracy'][k]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401dc82d-1859-4d38-85a9-5111dd5f618d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
